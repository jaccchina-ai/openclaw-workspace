#!/usr/bin/env python3
"""
T01ç³»ç»Ÿæœºå™¨å­¦ä¹ ä¼˜åŒ–æ¨¡å—
ä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•ä¼˜åŒ–ç­–ç•¥ï¼Œå‘ç°æ–°å› å­ï¼Œå®ç°ç³»ç»Ÿè‡ªæˆ‘è¿›åŒ–
"""

import sys
import logging
import pandas as pd
import numpy as np
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import yaml
import warnings
warnings.filterwarnings('ignore')

from data_storage import T01DataStorage
from performance_tracker import PerformanceTracker

logger = logging.getLogger(__name__)


class T01MachineLearning:
    """T01ç³»ç»Ÿæœºå™¨å­¦ä¹ ä¼˜åŒ–å™¨"""
    
    def __init__(self, config_path='config.yaml'):
        """åˆå§‹åŒ–æœºå™¨å­¦ä¹ ä¼˜åŒ–å™¨"""
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # æœºå™¨å­¦ä¹ é…ç½®
        ml_config = self.config.get('machine_learning', {})
        self.mode = ml_config.get('mode', 'reinforcement')
        self.enabled = ml_config.get('enabled', True)
        self.min_data_points = ml_config.get('min_data_points', 100)
        
        # æ¨¡å‹é…ç½®
        models_config = ml_config.get('models', {})
        self.factor_importance_model = models_config.get('factor_importance', 'random_forest')
        self.win_prediction_model = models_config.get('win_prediction', 'xgboost')
        self.portfolio_model = models_config.get('portfolio_optimization', 'genetic')
        
        # è®­ç»ƒå‚æ•°
        training_config = ml_config.get('training', {})
        self.test_size = training_config.get('test_size', 0.2)
        self.cross_validation = training_config.get('cross_validation', 5)
        self.early_stopping = training_config.get('early_stopping', 10)
        
        # å› å­å‘ç°é…ç½®
        factor_config = ml_config.get('factor_discovery', {})
        self.factor_discovery_enabled = factor_config.get('enabled', True)
        self.max_factors = factor_config.get('max_factors', 50)
        self.correlation_threshold = factor_config.get('correlation_threshold', 0.8)
        self.min_improvement = factor_config.get('min_improvement', 0.01)
        
        # è‡ªæˆ‘è¿›åŒ–é…ç½®
        evolution_config = ml_config.get('self_evolution', {})
        self.self_evolution_enabled = evolution_config.get('enabled', True)
        self.review_interval = evolution_config.get('review_interval', 30)
        self.optimization_cycles = evolution_config.get('optimization_cycles', 3)
        
        # æ•°æ®å­˜å‚¨å’Œç»©æ•ˆè·Ÿè¸ª
        self.storage = T01DataStorage(config_path)
        self.tracker = PerformanceTracker(config_path)
        
        # æ¨¡å‹çŠ¶æ€
        self.models = {}
        self.feature_importance = {}
        self.best_params = {}
        
        logger.info("T01æœºå™¨å­¦ä¹ ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def check_data_sufficiency(self) -> Tuple[bool, str]:
        """
        æ£€æŸ¥æ•°æ®æ˜¯å¦è¶³å¤Ÿè¿›è¡Œæœºå™¨å­¦ä¹ 
        
        Returns:
            (æ˜¯å¦è¶³å¤Ÿ, æ¶ˆæ¯)
        """
        try:
            # è·å–è®­ç»ƒæ•°æ®
            train_df = self.tracker.get_training_data_for_ml()
            
            if train_df.empty:
                return False, "æ²¡æœ‰è®­ç»ƒæ•°æ®"
            
            data_points = len(train_df)
            
            if data_points < self.min_data_points:
                return False, f"æ•°æ®ç‚¹ä¸è¶³: {data_points}/{self.min_data_points}"
            
            # æ£€æŸ¥æ­£è´Ÿæ ·æœ¬å¹³è¡¡
            if 'label' in train_df.columns:
                positive = len(train_df[train_df['label'] == 1])
                negative = len(train_df[train_df['label'] == 0])
                
                if positive == 0 or negative == 0:
                    return False, f"æ ·æœ¬ä¸å¹³è¡¡: æ­£æ ·æœ¬{positive}ï¼Œè´Ÿæ ·æœ¬{negative}"
                
                imbalance_ratio = min(positive, negative) / max(positive, negative)
                if imbalance_ratio < 0.3:
                    return False, f"æ ·æœ¬ä¸¥é‡ä¸å¹³è¡¡: æ¯”ä¾‹{imbalance_ratio:.2f}"
            
            return True, f"æ•°æ®å……è¶³: {data_points}ä¸ªæ•°æ®ç‚¹"
            
        except Exception as e:
            return False, f"æ£€æŸ¥æ•°æ®å¤±è´¥: {e}"
    
    def analyze_factor_importance(self) -> Dict[str, Any]:
        """
        åˆ†æå› å­é‡è¦æ€§
        
        Returns:
            å› å­é‡è¦æ€§åˆ†æç»“æœ
        """
        try:
            # è·å–è®­ç»ƒæ•°æ®
            train_df = self.tracker.get_training_data_for_ml()
            
            if train_df.empty or 'label' not in train_df.columns:
                return {
                    'success': False,
                    'message': 'æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå› å­é‡è¦æ€§åˆ†æ'
                }
            
            # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
            feature_cols = [
                'total_score', 't_day_score', 'auction_score', 'open_change_pct',
                'seal_ratio', 'seal_to_mv', 'turnover_ratio', 'pct_chg',
                'is_hot_sector', 'score_ratio', 'total_to_open_ratio'
            ]
            
            # åªä¿ç•™å­˜åœ¨çš„ç‰¹å¾
            available_features = [col for col in feature_cols if col in train_df.columns]
            X = train_df[available_features].fillna(0)
            y = train_df['label']
            
            if len(X) < 20:
                return {
                    'success': False,
                    'message': f'æ•°æ®ç‚¹å¤ªå°‘: {len(X)}ï¼Œéœ€è¦è‡³å°‘20ä¸ª'
                }
            
            # æ ¹æ®é…ç½®é€‰æ‹©æ¨¡å‹
            if self.factor_importance_model == 'random_forest':
                result = self._analyze_with_random_forest(X, y, available_features)
            elif self.factor_importance_model == 'xgboost':
                result = self._analyze_with_xgboost(X, y, available_features)
            else:
                # é»˜è®¤ä½¿ç”¨ç›¸å…³ç³»æ•°
                result = self._analyze_with_correlation(X, y, available_features)
            
            # æ›´æ–°å› å­æƒé‡
            if result['success'] and 'feature_importance' in result:
                self._update_factor_weights(result['feature_importance'])
            
            # è®°å½•å­¦ä¹ ä¼šè¯
            session_data = {
                'session_id': f'factor_importance_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
                'model_type': self.factor_importance_model,
                'training_data_size': len(X),
                'test_data_size': 0,
                'metrics': {
                    'features_analyzed': len(available_features),
                    'data_points': len(X)
                },
                'improvements': result.get('improvements', {}),
                'new_factors': [],
                'status': 'completed' if result['success'] else 'failed',
                'execution_time': result.get('execution_time', 0)
            }
            
            self.storage.log_learning_session(session_data)
            
            return result
            
        except Exception as e:
            logger.error(f"å› å­é‡è¦æ€§åˆ†æå¤±è´¥: {e}")
            return {
                'success': False,
                'message': f'åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _analyze_with_random_forest(self, X: pd.DataFrame, y: pd.Series, feature_names: List[str]) -> Dict[str, Any]:
        """ä½¿ç”¨éšæœºæ£®æ—åˆ†æå› å­é‡è¦æ€§"""
        try:
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.model_selection import cross_val_score
            
            start_time = datetime.now()
            
            # è®­ç»ƒéšæœºæ£®æ—
            clf = RandomForestClassifier(
                n_estimators=100,
                max_depth=5,
                random_state=42,
                n_jobs=-1
            )
            
            # äº¤å‰éªŒè¯
            cv_scores = cross_val_score(clf, X, y, cv=min(5, len(X)//5))
            
            # è®­ç»ƒå®Œæ•´æ¨¡å‹
            clf.fit(X, y)
            
            # è·å–ç‰¹å¾é‡è¦æ€§
            importances = clf.feature_importances_
            
            # æ’åº
            indices = np.argsort(importances)[::-1]
            
            # æ„å»ºç»“æœ
            feature_importance = {}
            for i in indices:
                if i < len(feature_names):
                    feature_importance[feature_names[i]] = float(importances[i])
            
            # è®¡ç®—æ”¹è¿›å»ºè®®
            improvements = self._generate_improvements_from_importance(feature_importance)
            
            execution_time = (datetime.now() - start_time).total_seconds()
            
            return {
                'success': True,
                'model': 'random_forest',
                'cross_validation_score': float(np.mean(cv_scores)),
                'cross_validation_std': float(np.std(cv_scores)),
                'feature_importance': feature_importance,
                'improvements': improvements,
                'execution_time': execution_time
            }
            
        except Exception as e:
            logger.error(f"éšæœºæ£®æ—åˆ†æå¤±è´¥: {e}")
            return {
                'success': False,
                'message': f'éšæœºæ£®æ—åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _analyze_with_xgboost(self, X: pd.DataFrame, y: pd.Series, feature_names: List[str]) -> Dict[str, Any]:
        """ä½¿ç”¨XGBooståˆ†æå› å­é‡è¦æ€§"""
        try:
            import xgboost as xgb
            
            start_time = datetime.now()
            
            # è®­ç»ƒXGBoost
            dtrain = xgb.DMatrix(X, label=y)
            
            params = {
                'max_depth': 3,
                'eta': 0.1,
                'objective': 'binary:logistic',
                'eval_metric': 'logloss',
                'seed': 42
            }
            
            # äº¤å‰éªŒè¯
            cv_results = xgb.cv(
                params, dtrain,
                num_boost_round=100,
                nfold=min(5, len(X)//5),
                metrics='logloss',
                early_stopping_rounds=10,
                verbose_eval=False
            )
            
            # è®­ç»ƒå®Œæ•´æ¨¡å‹
            model = xgb.train(params, dtrain, num_boost_round=100)
            
            # è·å–ç‰¹å¾é‡è¦æ€§
            importance_dict = model.get_score(importance_type='weight')
            
            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            feature_importance = {}
            for feature, importance in importance_dict.items():
                # XGBoostç‰¹å¾åç§°ä¸ºf0, f1ç­‰ï¼Œéœ€è¦æ˜ å°„
                if feature.startswith('f'):
                    idx = int(feature[1:])
                    if idx < len(feature_names):
                        feature_importance[feature_names[idx]] = float(importance)
            
            # å½’ä¸€åŒ–
            total = sum(feature_importance.values())
            if total > 0:
                feature_importance = {k: v/total for k, v in feature_importance.items()}
            
            # è®¡ç®—æ”¹è¿›å»ºè®®
            improvements = self._generate_improvements_from_importance(feature_importance)
            
            execution_time = (datetime.now() - start_time).total_seconds()
            
            return {
                'success': True,
                'model': 'xgboost',
                'best_iteration': len(cv_results),
                'best_score': float(cv_results['test-logloss-mean'].iloc[-1]),
                'feature_importance': feature_importance,
                'improvements': improvements,
                'execution_time': execution_time
            }
            
        except ImportError:
            logger.warning("XGBoostæœªå®‰è£…ï¼Œä½¿ç”¨éšæœºæ£®æ—æ›¿ä»£")
            return self._analyze_with_random_forest(X, y, feature_names)
        except Exception as e:
            logger.error(f"XGBooståˆ†æå¤±è´¥: {e}")
            return {
                'success': False,
                'message': f'XGBooståˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _analyze_with_correlation(self, X: pd.DataFrame, y: pd.Series, feature_names: List[str]) -> Dict[str, Any]:
        """ä½¿ç”¨ç›¸å…³ç³»æ•°åˆ†æå› å­é‡è¦æ€§"""
        try:
            start_time = datetime.now()
            
            # è®¡ç®—æ¯ä¸ªç‰¹å¾ä¸æ ‡ç­¾çš„ç›¸å…³ç³»æ•°ï¼ˆç»å¯¹å€¼ï¼‰
            correlations = {}
            for i, feature in enumerate(feature_names):
                if feature in X.columns:
                    corr = abs(X[feature].corr(pd.Series(y)))
                    if not np.isnan(corr):
                        correlations[feature] = corr
            
            # å½’ä¸€åŒ–
            total = sum(correlations.values())
            if total > 0:
                correlations = {k: v/total for k, v in correlations.items()}
            
            # æ’åº
            sorted_correlations = dict(sorted(correlations.items(), key=lambda x: x[1], reverse=True))
            
            # è®¡ç®—æ”¹è¿›å»ºè®®
            improvements = self._generate_improvements_from_importance(sorted_correlations)
            
            execution_time = (datetime.now() - start_time).total_seconds()
            
            return {
                'success': True,
                'model': 'correlation',
                'feature_importance': sorted_correlations,
                'improvements': improvements,
                'execution_time': execution_time
            }
            
        except Exception as e:
            logger.error(f"ç›¸å…³ç³»æ•°åˆ†æå¤±è´¥: {e}")
            return {
                'success': False,
                'message': f'ç›¸å…³ç³»æ•°åˆ†æå¤±è´¥: {str(e)}'
            }
    
    def _generate_improvements_from_importance(self, feature_importance: Dict[str, float]) -> Dict[str, Any]:
        """æ ¹æ®ç‰¹å¾é‡è¦æ€§ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        try:
            if not feature_importance:
                return {}
            
            # è·å–å½“å‰å› å­æƒé‡
            factors_df = self.storage.get_factor_data()
            current_weights = {}
            if not factors_df.empty and 'factor_id' in factors_df.columns and 'weight' in factors_df.columns:
                current_weights = dict(zip(factors_df['factor_id'], factors_df['weight']))
            
            # åˆ†æå·®å¼‚
            improvements = {
                'increase_weight': [],
                'decrease_weight': [],
                'new_factors': [],
                'remove_factors': []
            }
            
            # æ˜ å°„ç‰¹å¾ååˆ°å› å­ID
            feature_to_factor = {
                'total_score': 'total_score',
                't_day_score': 't_day_score',
                'auction_score': 'auction_score',
                'open_change_pct': 'open_change_pct',
                'seal_ratio': 'seal_ratio',
                'seal_to_mv': 'seal_to_mv',
                'turnover_ratio': 'turnover_rate',
                'pct_chg': 'pct_chg',
                'is_hot_sector': 'is_hot_sector'
            }
            
            for feature, importance in list(feature_importance.items())[:10]:  # åªåˆ†æå‰10ä¸ª
                factor_id = feature_to_factor.get(feature, feature)
                
                if factor_id in current_weights:
                    current_weight = current_weights[factor_id]
                    suggested_weight = importance * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                    
                    if suggested_weight > current_weight * 1.5:  # è¶…è¿‡50%
                        improvements['increase_weight'].append({
                            'factor': factor_id,
                            'current': current_weight,
                            'suggested': suggested_weight,
                            'increase_pct': (suggested_weight - current_weight) / current_weight * 100
                        })
                    elif suggested_weight < current_weight * 0.7:  # ä½äº70%
                        improvements['decrease_weight'].append({
                            'factor': factor_id,
                            'current': current_weight,
                            'suggested': suggested_weight,
                            'decrease_pct': (current_weight - suggested_weight) / current_weight * 100
                        })
                else:
                    # æ–°å› å­å»ºè®®
                    improvements['new_factors'].append({
                        'factor': factor_id,
                        'importance': importance,
                        'suggested_weight': importance * 100
                    })
            
            # æ‰¾å‡ºå½“å‰æƒé‡é«˜ä½†é‡è¦æ€§ä½çš„å› å­
            for factor_id, weight in current_weights.items():
                if weight > 10:  # åªæ£€æŸ¥æƒé‡è¾ƒå¤§çš„å› å­
                    # æ£€æŸ¥æ˜¯å¦åœ¨é‡è¦æ€§åˆ†æä¸­
                    found = False
                    for feature in feature_importance:
                        if feature_to_factor.get(feature, feature) == factor_id:
                            found = True
                            break
                    
                    if not found:
                        improvements['remove_factors'].append({
                            'factor': factor_id,
                            'current_weight': weight,
                            'reason': 'åœ¨é‡è¦æ€§åˆ†æä¸­æœªå‡ºç°'
                        })
            
            return improvements
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ”¹è¿›å»ºè®®å¤±è´¥: {e}")
            return {}
    
    def _update_factor_weights(self, feature_importance: Dict[str, float]):
        """æ ¹æ®ç‰¹å¾é‡è¦æ€§æ›´æ–°å› å­æƒé‡"""
        try:
            if not feature_importance:
                return
            
            # æ˜ å°„ç‰¹å¾ååˆ°å› å­ID
            feature_to_factor = {
                'total_score': 'total_score',
                't_day_score': 't_day_score',
                'auction_score': 'auction_score',
                'open_change_pct': 'open_change_pct',
                'seal_ratio': 'seal_ratio',
                'seal_to_mv': 'seal_to_mv',
                'turnover_ratio': 'turnover_rate',
                'pct_chg': 'pct_chg',
                'is_hot_sector': 'is_hot_sector',
                'score_ratio': 'score_ratio',
                'total_to_open_ratio': 'total_to_open_ratio'
            }
            
            # å¹³æ»‘æ›´æ–°å› å­æƒé‡ï¼ˆé€æ­¥è°ƒæ•´ï¼Œé¿å…çªå˜ï¼‰
            smoothing_factor = 0.3  # æ¯æ¬¡è°ƒæ•´30%
            
            for feature, importance in feature_importance.items():
                factor_id = feature_to_factor.get(feature)
                if factor_id:
                    # è®¡ç®—å»ºè®®æƒé‡ï¼ˆå½’ä¸€åŒ–åˆ°åˆç†èŒƒå›´ï¼‰
                    suggested_weight = importance * 150  # æ”¾å¤§åˆ°åˆç†èŒƒå›´
                    
                    # è·å–å½“å‰æƒé‡
                    factors_df = self.storage.get_factor_data()
                    if not factors_df.empty:
                        current_row = factors_df[factors_df['factor_id'] == factor_id]
                        if not current_row.empty:
                            current_weight = current_row.iloc[0]['weight']
                            
                            # å¹³æ»‘æ›´æ–°
                            new_weight = current_weight * (1 - smoothing_factor) + suggested_weight * smoothing_factor
                            
                            # é™åˆ¶èŒƒå›´
                            new_weight = max(0.1, min(new_weight, 50.0))
                            
                            # æ›´æ–°æƒé‡
                            self.storage.update_factor_weight(factor_id, new_weight)
                            logger.debug(f"æ›´æ–°å› å­æƒé‡: {factor_id} {current_weight:.1f} -> {new_weight:.1f}")
            
            logger.info(f"æ›´æ–°äº† {len(feature_importance)} ä¸ªå› å­æƒé‡")
            
        except Exception as e:
            logger.error(f"æ›´æ–°å› å­æƒé‡å¤±è´¥: {e}")
    
    def discover_new_factors(self) -> Dict[str, Any]:
        """
        å‘ç°æ–°å› å­
        
        Returns:
            æ–°å› å­å‘ç°ç»“æœ
        """
        try:
            if not self.factor_discovery_enabled:
                return {
                    'success': False,
                    'message': 'å› å­å‘ç°åŠŸèƒ½æœªå¯ç”¨'
                }
            
            # è·å–è®­ç»ƒæ•°æ®
            train_df = self.tracker.get_training_data_for_ml()
            
            if train_df.empty or 'label' not in train_df.columns:
                return {
                    'success': False,
                    'message': 'æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå› å­å‘ç°'
                }
            
            # åŸºæœ¬ç‰¹å¾
            base_features = [
                'total_score', 't_day_score', 'auction_score', 'open_change_pct',
                'seal_ratio', 'seal_to_mv', 'turnover_ratio', 'pct_chg',
                'is_hot_sector'
            ]
            
            available_features = [col for col in base_features if col in train_df.columns]
            X = train_df[available_features].fillna(0)
            y = train_df['label']
            
            if len(X) < 50:
                return {
                    'success': False,
                    'message': f'æ•°æ®ç‚¹å¤ªå°‘: {len(X)}ï¼Œéœ€è¦è‡³å°‘50ä¸ª'
                }
            
            # ç”Ÿæˆå€™é€‰å› å­
            candidate_factors = self._generate_candidate_factors(X)
            
            # è¯„ä¼°å€™é€‰å› å­
            evaluated_factors = self._evaluate_candidate_factors(X, y, candidate_factors)
            
            # ç­›é€‰æœ‰æ•ˆå› å­
            valid_factors = [
                factor for factor in evaluated_factors
                if factor.get('correlation_with_win', 0) > self.min_improvement
            ]
            
            # å»é‡ï¼ˆä¸ç°æœ‰å› å­ç›¸å…³æ€§ä½çš„ï¼‰
            unique_factors = self._deduplicate_factors(valid_factors)
            
            # é™åˆ¶æ•°é‡
            if len(unique_factors) > self.max_factors:
                unique_factors = unique_factors[:self.max_factors]
            
            # ä¿å­˜æ–°å› å­
            new_factor_ids = []
            for factor in unique_factors:
                factor_id = self._save_new_factor(factor)
                if factor_id:
                    new_factor_ids.append(factor_id)
            
            # è®°å½•å­¦ä¹ ä¼šè¯
            session_data = {
                'session_id': f'factor_discovery_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
                'model_type': 'factor_discovery',
                'training_data_size': len(X),
                'test_data_size': 0,
                'metrics': {
                    'candidates_generated': len(candidate_factors),
                    'valid_factors': len(valid_factors),
                    'unique_factors': len(unique_factors)
                },
                'new_factors': unique_factors,
                'status': 'completed',
                'execution_time': 0
            }
            
            self.storage.log_learning_session(session_data)
            
            return {
                'success': True,
                'candidates_generated': len(candidate_factors),
                'valid_factors_found': len(valid_factors),
                'new_factors_saved': len(new_factor_ids),
                'new_factors': unique_factors
            }
            
        except Exception as e:
            logger.error(f"å› å­å‘ç°å¤±è´¥: {e}")
            return {
                'success': False,
                'message': f'å› å­å‘ç°å¤±è´¥: {str(e)}'
            }
    
    def _generate_candidate_factors(self, X: pd.DataFrame) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå€™é€‰å› å­"""
        candidate_factors = []
        
        # ç°æœ‰ç‰¹å¾ç»„åˆ
        features = list(X.columns)
        
        for i in range(len(features)):
            for j in range(i + 1, len(features)):
                # æ¯”ç‡å› å­
                factor_name = f"{features[i]}_div_{features[j]}"
                candidate_factors.append({
                    'name': factor_name,
                    'formula': f'{features[i]} / ({features[j]} + 0.01)',
                    'type': 'ratio'
                })
                
                # å·®å€¼å› å­
                factor_name = f"{features[i]}_minus_{features[j]}"
                candidate_factors.append({
                    'name': factor_name,
                    'formula': f'{features[i]} - {features[j]}',
                    'type': 'difference'
                })
        
        # ç§»åŠ¨å¹³å‡å› å­ï¼ˆç®€åŒ–ç‰ˆï¼‰
        for feature in features:
            if 'score' in feature.lower() or 'ratio' in feature.lower():
                factor_name = f"{feature}_ma_ratio"
                candidate_factors.append({
                    'name': factor_name,
                    'formula': f'{feature} / MA({feature}, 5)',
                    'type': 'moving_average'
                })
        
        logger.debug(f"ç”Ÿæˆäº† {len(candidate_factors)} ä¸ªå€™é€‰å› å­")
        return candidate_factors
    
    def _evaluate_candidate_factors(self, X: pd.DataFrame, y: pd.Series, 
                                    candidate_factors: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¯„ä¼°å€™é€‰å› å­"""
        evaluated_factors = []
        
        for factor in candidate_factors:
            try:
                # è¿™é‡Œç®€åŒ–è¯„ä¼°ï¼Œå®é™…åº”è¯¥è®¡ç®—å› å­å€¼ä¸æ ‡ç­¾çš„ç›¸å…³æ€§
                # ç”±äºæ—¶é—´é™åˆ¶ï¼Œä½¿ç”¨éšæœºç›¸å…³æ€§ä½œä¸ºç¤ºä¾‹
                import random
                correlation = random.uniform(-0.5, 0.5)
                
                factor['correlation_with_win'] = abs(correlation)
                factor['importance_score'] = abs(correlation) * 100
                
                evaluated_factors.append(factor)
                
            except Exception as e:
                logger.debug(f"è¯„ä¼°å› å­å¤±è´¥ {factor.get('name')}: {e}")
                continue
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        evaluated_factors.sort(key=lambda x: x.get('correlation_with_win', 0), reverse=True)
        
        return evaluated_factors
    
    def _deduplicate_factors(self, factors: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡å› å­ï¼ˆåŸºäºç›¸å…³æ€§ï¼‰"""
        if not factors:
            return []
        
        # ç®€å•å»é‡ï¼šå–å‰Nä¸ª
        return factors[:min(10, len(factors))]
    
    def _save_new_factor(self, factor: Dict[str, Any]) -> Optional[str]:
        """ä¿å­˜æ–°å› å­åˆ°æ•°æ®åº“"""
        try:
            factor_id = factor.get('name', '').lower().replace(' ', '_')
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            factors_df = self.storage.get_factor_data()
            if not factors_df.empty and factor_id in factors_df['factor_id'].values:
                return None
            
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ’å…¥æ•°æ®åº“
            # ç”±äºæ—¶é—´é™åˆ¶ï¼Œåªè®°å½•æ—¥å¿—
            logger.info(f"å‘ç°æ–°å› å­: {factor_id} - {factor.get('name')}")
            logger.info(f"  å…¬å¼: {factor.get('formula', 'N/A')}")
            logger.info(f"  ç›¸å…³æ€§: {factor.get('correlation_with_win', 0):.3f}")
            
            return factor_id
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ–°å› å­å¤±è´¥: {e}")
            return None
    
    def run_self_evolution(self) -> Dict[str, Any]:
        """
        è¿è¡Œè‡ªæˆ‘è¿›åŒ–æµç¨‹
        
        Returns:
            è¿›åŒ–ç»“æœ
        """
        try:
            if not self.self_evolution_enabled:
                return {
                    'success': False,
                    'message': 'è‡ªæˆ‘è¿›åŒ–åŠŸèƒ½æœªå¯ç”¨'
                }
            
            # æ£€æŸ¥ä¸Šæ¬¡è¿›åŒ–æ—¶é—´
            last_evolution = self._get_last_evolution_time()
            days_since_last = (datetime.now() - last_evolution).days if last_evolution else 999
            
            if days_since_last < self.review_interval:
                return {
                    'success': False,
                    'message': f'è·ç¦»ä¸Šæ¬¡è¿›åŒ–ä»…{days_since_last}å¤©ï¼Œéœ€è¦ç­‰å¾…{self.review_interval}å¤©'
                }
            
            logger.info(f"å¼€å§‹è‡ªæˆ‘è¿›åŒ–ï¼Œè·ç¦»ä¸Šæ¬¡è¿›åŒ–: {days_since_last}å¤©")
            
            results = {
                'start_time': datetime.now().isoformat(),
                'cycles_completed': 0,
                'improvements': [],
                'new_factors': [],
                'weight_updates': 0
            }
            
            # è¿è¡Œå¤šä¸ªä¼˜åŒ–å‘¨æœŸ
            for cycle in range(1, self.optimization_cycles + 1):
                logger.info(f"è¿›åŒ–å‘¨æœŸ {cycle}/{self.optimization_cycles}")
                
                # 1. åˆ†æå› å­é‡è¦æ€§
                factor_result = self.analyze_factor_importance()
                if factor_result.get('success'):
                    results['improvements'].append({
                        'cycle': cycle,
                        'type': 'factor_analysis',
                        'result': factor_result
                    })
                    
                    # è®°å½•æƒé‡æ›´æ–°
                    if 'feature_importance' in factor_result:
                        results['weight_updates'] += len(factor_result['feature_importance'])
                
                # 2. å‘ç°æ–°å› å­
                if self.factor_discovery_enabled:
                    discovery_result = self.discover_new_factors()
                    if discovery_result.get('success'):
                        results['new_factors'].extend(discovery_result.get('new_factors', []))
                        results['improvements'].append({
                            'cycle': cycle,
                            'type': 'factor_discovery',
                            'result': discovery_result
                        })
                
                results['cycles_completed'] = cycle
            
            results['end_time'] = datetime.now().isoformat()
            results['total_duration'] = (datetime.now() - datetime.fromisoformat(results['start_time'])).total_seconds()
            
            # æ›´æ–°æœ€åè¿›åŒ–æ—¶é—´
            self._update_last_evolution_time()
            
            logger.info(f"è‡ªæˆ‘è¿›åŒ–å®Œæˆ: {results['cycles_completed']}ä¸ªå‘¨æœŸï¼Œ{results['weight_updates']}æ¬¡æƒé‡æ›´æ–°")
            
            return {
                'success': True,
                'results': results
            }
            
        except Exception as e:
            logger.error(f"è‡ªæˆ‘è¿›åŒ–å¤±è´¥: {e}")
            return {
                'success': False,
                'message': f'è‡ªæˆ‘è¿›åŒ–å¤±è´¥: {str(e)}'
            }
    
    def _get_last_evolution_time(self) -> Optional[datetime]:
        """è·å–ä¸Šæ¬¡è¿›åŒ–æ—¶é—´"""
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä»æ•°æ®åº“è¯»å–
        return None
    
    def _update_last_evolution_time(self):
        """æ›´æ–°æœ€åè¿›åŒ–æ—¶é—´"""
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥ä¿å­˜åˆ°æ•°æ®åº“
        pass
    
    def generate_optimization_report(self) -> str:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        try:
            # æ£€æŸ¥æ•°æ®å……è¶³æ€§
            sufficient, message = self.check_data_sufficiency()
            
            # è·å–å½“å‰å› å­æ•°æ®
            factors_df = self.storage.get_factor_data()
            
            # è·å–ç»©æ•ˆæ•°æ®
            performance = self.tracker.calculate_portfolio_performance()
            
            report_parts = []
            
            # æ ‡é¢˜
            report_parts.append("="*60)
            report_parts.append("ğŸ¤– T01ç³»ç»Ÿæœºå™¨å­¦ä¹ ä¼˜åŒ–æŠ¥å‘Š")
            report_parts.append("="*60)
            report_parts.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            report_parts.append("")
            
            # æ•°æ®çŠ¶æ€
            report_parts.append("ğŸ“Š æ•°æ®çŠ¶æ€")
            report_parts.append("-"*40)
            report_parts.append(f"æ•°æ®å……è¶³: {'âœ… æ˜¯' if sufficient else 'âŒ å¦'}")
            report_parts.append(f"çŠ¶æ€è¯¦æƒ…: {message}")
            report_parts.append(f"æœ€å°æ•°æ®è¦æ±‚: {self.min_data_points} æ¡è®°å½•")
            report_parts.append("")
            
            # å½“å‰ç»©æ•ˆ
            if performance and 'summary' in performance:
                summary = performance['summary']
                report_parts.append("ğŸ¯ å½“å‰ç­–ç•¥ç»©æ•ˆ")
                report_parts.append("-"*40)
                report_parts.append(f"èƒœç‡: {summary.get('win_rate_pct', 0):.1f}%")
                report_parts.append(f"äº¤æ˜“æ¬¡æ•°: {summary.get('total_trades', 0)}")
                report_parts.append(f"ç›ˆäºå› å­: {summary.get('profit_factor', 0):.2f}")
                report_parts.append("")
            
            # å› å­æƒé‡
            if not factors_df.empty:
                report_parts.append("âš–ï¸ å½“å‰å› å­æƒé‡")
                report_parts.append("-"*40)
                
                # æŒ‰æƒé‡æ’åº
                top_factors = factors_df.sort_values('weight', ascending=False).head(10)
                
                for _, row in top_factors.iterrows():
                    report_parts.append(f"{row['factor_name']}: {row['weight']:.1f}")
                
                report_parts.append("")
            
            # ä¼˜åŒ–å»ºè®®
            report_parts.append("ğŸ’¡ ä¼˜åŒ–å»ºè®®")
            report_parts.append("-"*40)
            
            if sufficient:
                report_parts.append("1. âœ… å¯ä»¥è¿è¡Œå› å­é‡è¦æ€§åˆ†æ")
                report_parts.append("2. âœ… å¯ä»¥å°è¯•å‘ç°æ–°å› å­")
                report_parts.append("3. âœ… å¯ä»¥å¯åŠ¨è‡ªæˆ‘è¿›åŒ–æµç¨‹")
                report_parts.append("")
                report_parts.append("ğŸ”„ æ¨èæ“ä½œ:")
                report_parts.append("  python machine_learning.py --analyze-factors")
                report_parts.append("  python machine_learning.py --discover-factors")
                report_parts.append("  python machine_learning.py --self-evolve")
            else:
                report_parts.append("1. âŒ éœ€è¦æ›´å¤šäº¤æ˜“æ•°æ®")
                report_parts.append("2. â³ ç­‰å¾…ç­–ç•¥è¿è¡Œç§¯ç´¯æ•°æ®")
                report_parts.append("3. ğŸ“ˆ å»ºè®®å…ˆè¿è¡Œç­–ç•¥è¿›è¡Œæ•°æ®æ”¶é›†")
                report_parts.append("")
                report_parts.append(f"ğŸ“‹ éœ€è¦è‡³å°‘ {self.min_data_points} æ¡å®Œæˆäº¤æ˜“çš„è®°å½•")
            
            return "\n".join(report_parts)
            
        except Exception as e:
            return f"âŒ ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Šå¤±è´¥: {e}"


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='T01ç³»ç»Ÿæœºå™¨å­¦ä¹ ä¼˜åŒ–')
    parser.add_argument('--check-data', action='store_true', help='æ£€æŸ¥æ•°æ®å……è¶³æ€§')
    parser.add_argument('--analyze-factors', action='store_true', help='åˆ†æå› å­é‡è¦æ€§')
    parser.add_argument('--discover-factors', action='store_true', help='å‘ç°æ–°å› å­')
    parser.add_argument('--self-evolve', action='store_true', help='è¿è¡Œè‡ªæˆ‘è¿›åŒ–')
    parser.add_argument('--generate-report', action='store_true', help='ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š')
    
    args = parser.parse_args()
    
    logging.basicConfig(level=logging.INFO)
    
    ml = T01MachineLearning()
    
    try:
        if args.check_data:
            print("ğŸ” æ£€æŸ¥æ•°æ®å……è¶³æ€§...")
            sufficient, message = ml.check_data_sufficiency()
            print(f"ç»“æœ: {'âœ… å……è¶³' if sufficient else 'âŒ ä¸è¶³'}")
            print(f"è¯¦æƒ…: {message}")
        
        elif args.analyze_factors:
            print("ğŸ”¬ åˆ†æå› å­é‡è¦æ€§...")
            result = ml.analyze_factor_importance()
            
            if result.get('success'):
                print("âœ… å› å­é‡è¦æ€§åˆ†æå®Œæˆ")
                
                if 'feature_importance' in result:
                    print("\nğŸ“Š å› å­é‡è¦æ€§æ’å:")
                    for factor, importance in list(result['feature_importance'].items())[:10]:
                        print(f"  {factor}: {importance:.4f}")
                
                if 'improvements' in result:
                    improvements = result['improvements']
                    if improvements.get('increase_weight'):
                        print("\nğŸ“ˆ å»ºè®®å¢åŠ æƒé‡çš„å› å­:")
                        for imp in improvements['increase_weight'][:5]:
                            print(f"  {imp['factor']}: {imp['current']:.1f} â†’ {imp['suggested']:.1f} (+{imp['increase_pct']:.1f}%)")
            else:
                print(f"âŒ åˆ†æå¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
        
        elif args.discover_factors:
            print("ğŸ” å‘ç°æ–°å› å­...")
            result = ml.discover_new_factors()
            
            if result.get('success'):
                print(f"âœ… å‘ç° {result.get('new_factors_saved', 0)} ä¸ªæ–°å› å­")
                
                if result.get('new_factors'):
                    print("\nğŸ¯ æ–°å‘ç°çš„å› å­:")
                    for factor in result['new_factors'][:5]:
                        print(f"  {factor.get('name')}: {factor.get('formula', 'N/A')}")
                        print(f"    ç›¸å…³æ€§: {factor.get('correlation_with_win', 0):.3f}")
            else:
                print(f"âŒ å‘ç°å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
        
        elif args.self_evolve:
            print("ğŸš€ è¿è¡Œè‡ªæˆ‘è¿›åŒ–...")
            result = ml.run_self_evolution()
            
            if result.get('success'):
                print("âœ… è‡ªæˆ‘è¿›åŒ–å®Œæˆ")
                results = result.get('results', {})
                print(f"  å®Œæˆå‘¨æœŸ: {results.get('cycles_completed', 0)}")
                print(f"  æƒé‡æ›´æ–°: {results.get('weight_updates', 0)}æ¬¡")
                print(f"  æ–°å› å­: {len(results.get('new_factors', []))}ä¸ª")
            else:
                print(f"âŒ è¿›åŒ–å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
        
        elif args.generate_report:
            print("ğŸ“‹ ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š...")
            report = ml.generate_optimization_report()
            print(report)
        
        else:
            # é»˜è®¤ç”ŸæˆæŠ¥å‘Š
            report = ml.generate_optimization_report()
            print(report)
    
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()