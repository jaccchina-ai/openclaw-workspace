#!/usr/bin/env python3
"""
T01 èˆ†æƒ…åˆ†ææµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯æ–°é—»èˆ†æƒ…åˆ†æå¯¹æ¶¨åœè‚¡çƒ­åº¦æŒç»­æ€§åˆ¤æ–­çš„ä»·å€¼
"""

import os
import json
import subprocess
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NewsSentimentAnalyzer:
    """æ–°é—»èˆ†æƒ…åˆ†æå™¨ - ä½¿ç”¨Tavily API"""
    
    def __init__(self, tavily_api_key: Optional[str] = None):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.api_key = tavily_api_key or os.environ.get('TAVILY_API_KEY')
        if not self.api_key:
            raise ValueError("TAVILY_API_KEY not found in environment variables")
        
        self.tavily_script_path = "/root/.openclaw/workspace/skills/tavily-search/scripts/search.mjs"
        if not os.path.exists(self.tavily_script_path):
            raise FileNotFoundError(f"Tavily script not found: {self.tavily_script_path}")
        
        logger.info("NewsSentimentAnalyzer initialized with Tavily API")
    
    def search_stock_news(self, stock_name: str, stock_code: str, trade_date: str, 
                         days_back: int = 1) -> Dict[str, Any]:
        """
        æœç´¢è‚¡ç¥¨ç›¸å…³æ–°é—»
        
        Args:
            stock_name: è‚¡ç¥¨åç§°
            stock_code: è‚¡ç¥¨ä»£ç 
            trade_date: äº¤æ˜“æ—¥æœŸ (YYYYMMDD)
            days_back: å›æº¯å¤©æ•°
            
        Returns:
            æ–°é—»æœç´¢ç»“æœ
        """
        # æ„å»ºæœç´¢æŸ¥è¯¢ - ä¼˜åŒ–ç‰ˆ (æ·»åŠ ä¸­æ–‡é™å®šè¯å’Œå¸‚åœºæ ‡è¯†)
        # æå–å¸‚åœºæ ‡è¯†
        market = "Aè‚¡"
        if stock_code.endswith('.SH'):
            market = "æ²ªå¸‚"
        elif stock_code.endswith('.SZ'):
            market = "æ·±å¸‚"
        
        queries = [
            f"{stock_name} {stock_code} {market} ä¸­å›½ Aè‚¡ æ¶¨åœæ¿ æ¶¨åœ",  # æ¶¨åœç›¸å…³
            f"{stock_name} {stock_code} {market} ä¸­å›½ Aè‚¡ åˆ©å¥½ æ¶ˆæ¯",  # åˆ©å¥½ç›¸å…³
            f"{stock_name} {stock_code} {market} ä¸­å›½ Aè‚¡ ä¸šç»© é¢„å‘Š",  # ä¸šç»©ç›¸å…³
            f"{stock_name} {stock_code} {market} ä¸­å›½ Aè‚¡ å…¬å‘Š é€šçŸ¥",  # å…¬å¸å…¬å‘Š
        ]
        
        all_results = []
        for query in queries:
            try:
                result = self._call_tavily(query, days_back=days_back)
                if result and result.get('sources'):
                    all_results.extend(result['sources'])
                    logger.info(f"Query '{query}' returned {len(result.get('sources', []))} results")
            except Exception as e:
                logger.warning(f"Query '{query}' failed: {e}")
                continue
        
        # å»é‡
        unique_results = self._deduplicate_results(all_results)
        
        return {
            'stock_name': stock_name,
            'stock_code': stock_code,
            'trade_date': trade_date,
            'total_news_count': len(unique_results),
            'news_results': unique_results[:10],  # åªè¿”å›å‰10æ¡
            'search_queries': queries
        }
    
    def _call_tavily(self, query: str, days_back: int = 1) -> Dict[str, Any]:
        """è°ƒç”¨Tavily API"""
        cmd = [
            'node', self.tavily_script_path,
            query,
            '--topic', 'news',
            '--days', str(days_back),
            '-n', '5'
        ]
        
        logger.debug(f"Executing Tavily command: {' '.join(cmd)}")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env['TAVILY_API_KEY'] = self.api_key
        
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                env=env,
                timeout=30  # 30ç§’è¶…æ—¶
            )
            
            if result.returncode != 0:
                logger.error(f"Tavily command failed: {result.stderr}")
                return {'answer': '', 'sources': []}
            
            # è§£æè¾“å‡º
            return self._parse_tavily_output(result.stdout)
            
        except subprocess.TimeoutExpired:
            logger.error("Tavily command timeout")
            return {'answer': '', 'sources': []}
        except Exception as e:
            logger.error(f"Tavily command error: {e}")
            return {'answer': '', 'sources': []}
    
    def _parse_tavily_output(self, output: str) -> Dict[str, Any]:
        """è§£æTavilyè¾“å‡º"""
        lines = output.strip().split('\n')
        
        answer = ""
        sources = []
        current_source = {}
        
        in_answer_section = False
        in_sources_section = False
        
        for line in lines:
            line = line.strip()
            
            if line.startswith('## Answer'):
                in_answer_section = True
                in_sources_section = False
                continue
            elif line.startswith('## Sources'):
                in_answer_section = False
                in_sources_section = True
                continue
            elif line.startswith('---'):
                in_answer_section = False
                continue
            
            if in_answer_section and line:
                answer += line + ' '
            elif in_sources_section:
                if line.startswith('- **'):
                    # æ–°æ¥æºå¼€å§‹
                    if current_source:
                        sources.append(current_source)
                    
                    # æå–æ ‡é¢˜å’ŒURL
                    import re
                    title_match = re.search(r'\*\*(.*?)\*\*', line)
                    url_match = re.search(r'https?://[^\s)]+', line)
                    score_match = re.search(r'\(relevance: (\d+)%\)', line)
                    
                    current_source = {
                        'title': title_match.group(1) if title_match else '',
                        'url': url_match.group(0) if url_match else '',
                        'relevance_score': int(score_match.group(1)) / 100 if score_match else 0.5
                    }
                elif line.startswith('http') or line.startswith('  http'):
                    # URLè¡Œ - è®¾ç½®URL
                    url = line.strip()
                    if current_source:
                        current_source['url'] = url
                    # ä¸è§†ä¸ºå†…å®¹è¡Œï¼Œç»§ç»­å¾ªç¯
                    continue
                elif line and current_source:
                    # å†…å®¹è¡Œ
                    if 'content' not in current_source:
                        current_source['content'] = line
                    else:
                        current_source['content'] += ' ' + line
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ¥æº
        if current_source:
            sources.append(current_source)
        
        return {
            'answer': answer.strip(),
            'sources': sources
        }
    
    def _deduplicate_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡æ–°é—»ç»“æœ"""
        seen_urls = set()
        unique_results = []
        
        for result in results:
            url = result.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(result)
        
        return unique_results
    
    def analyze_sentiment(self, news_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ç®€å•æƒ…æ„Ÿåˆ†æï¼ˆå…³é”®è¯åŒ¹é…ï¼‰
        
        æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€å•å®ç°ï¼Œå®é™…åº”ä½¿ç”¨NLPæ¨¡å‹
        """
        # ä¸­æ–‡å…³é”®è¯
        positive_keywords_cn = ['åˆ©å¥½', 'å¢é•¿', 'è¶…é¢„æœŸ', 'çªç ´', 'åˆ›æ–°é«˜', 'æ¨è', 'ä¹°å…¥', 'çœ‹å¥½', 'ä¸Šæ¶¨', 'å¼ºåŠ¿']
        negative_keywords_cn = ['åˆ©ç©º', 'ä¸‹è·Œ', 'äºæŸ', 'é£é™©', 'å‡æŒ', 'å–å‡º', 'è°¨æ…', 'é¢„è­¦', 'æš´è·Œ', 'è°ƒæ•´']
        
        # è‹±æ–‡å…³é”®è¯
        positive_keywords_en = ['positive', 'growth', 'beat', 'outperform', 'buy', 'bullish', 'upgrade', 'strong', 'gain']
        negative_keywords_en = ['negative', 'decline', 'loss', 'risk', 'sell', 'bearish', 'downgrade', 'weak', 'drop', 'fall']
        
        # åˆå¹¶å…³é”®è¯
        positive_keywords = positive_keywords_cn + positive_keywords_en
        negative_keywords = negative_keywords_cn + negative_keywords_en
        
        total_score = 0
        positive_count = 0
        negative_count = 0
        neutral_count = 0
        
        for news in news_results:
            content = f"{news.get('title', '')} {news.get('content', '')}"
            content_lower = content.lower()
            
            # ç®€å•å…³é”®è¯åŒ¹é…
            pos_matches = sum(1 for keyword in positive_keywords if keyword in content)
            neg_matches = sum(1 for keyword in negative_keywords if keyword in content)
            
            if pos_matches > neg_matches:
                sentiment = 'positive'
                positive_count += 1
                score = 1.0
            elif neg_matches > pos_matches:
                sentiment = 'negative'
                negative_count += 1
                score = -1.0
            else:
                sentiment = 'neutral'
                neutral_count += 1
                score = 0.0
            
            news['sentiment'] = sentiment
            news['sentiment_score'] = score
            total_score += score
        
        total_news = len(news_results)
        if total_news > 0:
            avg_sentiment = total_score / total_news
            positive_ratio = positive_count / total_news
            negative_ratio = negative_count / total_news
        else:
            avg_sentiment = 0
            positive_ratio = 0
            negative_ratio = 0
        
        # è®¡ç®—èˆ†æƒ…ç»¼åˆè¯„åˆ† (0-10åˆ†)
        # åŸºäº: æ–°é—»æ•°é‡ + æƒ…æ„Ÿå€¾å‘ + æ­£é¢æ–°é—»æ¯”ä¾‹
        overall_score = 0
        if total_news > 0:
            # åŸºç¡€åˆ†: æœ‰æ–°é—»å…³æ³¨åº¦
            overall_score += 2.0
            
            # æ­£é¢æ–°é—»åŠ åˆ†
            if positive_count > 0:
                overall_score += min(positive_count * 0.5, 3.0)
            
            # æƒ…æ„Ÿå¾—åˆ†æ˜ å°„ (-1åˆ°1æ˜ å°„åˆ°0åˆ°3åˆ†)
            sentiment_mapped = (avg_sentiment + 1) * 1.5  # -1->0, 0->1.5, 1->3
            overall_score += min(sentiment_mapped, 3.0)
            
            # æ–°é—»æ•°é‡åŠ åˆ† (ä¸Šé™2åˆ†)
            news_count_score = min(total_news * 0.1, 2.0)
            overall_score += news_count_score
        
        # é™åˆ¶åœ¨0-10åˆ†
        overall_score = max(0, min(overall_score, 10))
        
        return {
            'total_news': total_news,
            'positive_count': positive_count,
            'negative_count': negative_count,
            'neutral_count': neutral_count,
            'positive_ratio': positive_ratio,
            'negative_ratio': negative_ratio,
            'avg_sentiment': avg_sentiment,
            'sentiment_category': 'positive' if avg_sentiment > 0.2 else 'negative' if avg_sentiment < -0.2 else 'neutral',
            'overall_sentiment': 'positive' if avg_sentiment > 0.2 else 'negative' if avg_sentiment < -0.2 else 'neutral',
            'overall_score': round(overall_score, 2)
        }
    
    def calculate_heat_index(self, news_count: int, avg_sentiment: float, 
                           recent_days: int = 3) -> float:
        """
        è®¡ç®—çƒ­åº¦æŒ‡æ•°
        
        å…¬å¼: (æ–°é—»æ•°é‡ Ã— æƒ…æ„Ÿå¾—åˆ† Ã— æ—¶é—´è¡°å‡å› å­)
        """
        # æ—¶é—´è¡°å‡å› å­ï¼šè¶Šè¿‘çš„æ–°é—»æƒé‡è¶Šé«˜
        time_factor = 1.0 / recent_days if recent_days > 0 else 1.0
        
        # åŸºç¡€çƒ­åº¦è®¡ç®—
        base_heat = news_count * (1 + avg_sentiment)  # avg_sentimentåœ¨-1åˆ°1ä¹‹é—´
        
        # åº”ç”¨æ—¶é—´è¡°å‡
        heat_index = base_heat * time_factor
        
        # å½’ä¸€åŒ–åˆ°0-100èŒƒå›´ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        normalized_heat = min(100, max(0, heat_index * 10))
        
        return round(normalized_heat, 2)


def test_with_sample_data():
    """ä½¿ç”¨æ ·æœ¬æ•°æ®æµ‹è¯•"""
    print("=== T01 èˆ†æƒ…åˆ†ææ¨¡å—æµ‹è¯• ===")
    
    # åˆå§‹åŒ–åˆ†æå™¨
    try:
        analyzer = NewsSentimentAnalyzer()
        print("âœ… NewsSentimentAnalyzer åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # æµ‹è¯•æ ·æœ¬ï¼šå®å¾·æ—¶ä»£ï¼ˆä¹‹å‰æµ‹è¯•è¿‡çš„æ¶¨åœè‚¡ï¼‰
    test_cases = [
        {
            'name': 'å®å¾·æ—¶ä»£',
            'code': '300750.SZ',
            'date': '20250224',
            'is_limit_up': True
        },
        {
            'name': 'è´µå·èŒ…å°',
            'code': '600519.SH',
            'date': '20250224',
            'is_limit_up': False  # å¯¹æ¯”éæ¶¨åœè‚¡
        }
    ]
    
    for test_case in test_cases:
        print(f"\nğŸ“Š æµ‹è¯•è‚¡ç¥¨: {test_case['name']} ({test_case['code']})")
        print(f"   äº¤æ˜“æ—¥æœŸ: {test_case['date']}")
        print(f"   æ˜¯å¦æ¶¨åœ: {'æ˜¯' if test_case['is_limit_up'] else 'å¦'}")
        
        # æœç´¢æ–°é—»
        print("   æœç´¢ç›¸å…³æ–°é—»...")
        news_result = analyzer.search_stock_news(
            stock_name=test_case['name'],
            stock_code=test_case['code'],
            trade_date=test_case['date'],
            days_back=3
        )
        
        print(f"   æ‰¾åˆ° {news_result['total_news_count']} æ¡ç›¸å…³æ–°é—»")
        
        # æƒ…æ„Ÿåˆ†æ
        if news_result['total_news_count'] > 0:
            sentiment_result = analyzer.analyze_sentiment(news_result['news_results'])
            
            print(f"   æƒ…æ„Ÿåˆ†æ:")
            print(f"     - æ­£é¢: {sentiment_result['positive_count']}æ¡ ({sentiment_result['positive_ratio']*100:.1f}%)")
            print(f"     - è´Ÿé¢: {sentiment_result['negative_count']}æ¡ ({sentiment_result['negative_ratio']*100:.1f}%)")
            print(f"     - ä¸­æ€§: {sentiment_result['neutral_count']}æ¡")
            print(f"     - å¹³å‡æƒ…æ„Ÿ: {sentiment_result['avg_sentiment']:.2f}")
            print(f"     - æƒ…æ„Ÿåˆ†ç±»: {sentiment_result['sentiment_category']}")
            
            # çƒ­åº¦æŒ‡æ•°
            heat_index = analyzer.calculate_heat_index(
                news_count=news_result['total_news_count'],
                avg_sentiment=sentiment_result['avg_sentiment']
            )
            
            print(f"   çƒ­åº¦æŒ‡æ•°: {heat_index}/100")
            
            # T01è¯„åˆ†å»ºè®®
            if test_case['is_limit_up']:
                if heat_index > 60:
                    suggestion = "âœ… é«˜çƒ­åº¦ï¼ŒæŒç»­æ€§å¼ºï¼Œå»ºè®®é‡ç‚¹å…³æ³¨"
                elif heat_index > 30:
                    suggestion = "âš ï¸ ä¸­ç­‰çƒ­åº¦ï¼Œéœ€ç»“åˆæŠ€æœ¯æŒ‡æ ‡åˆ¤æ–­"
                else:
                    suggestion = "âŒ ä½çƒ­åº¦ï¼Œè­¦æƒ•ä¸€æ—¥æ¸¸"
                
                print(f"   T01å»ºè®®: {suggestion}")
        
        # æ˜¾ç¤ºå‰3æ¡æ–°é—»
        if news_result['total_news_count'] > 0:
            print(f"\n   å‰3æ¡æ–°é—»æ‘˜è¦:")
            for i, news in enumerate(news_result['news_results'][:3], 1):
                print(f"     {i}. {news.get('title', 'æ— æ ‡é¢˜')}")
                if 'sentiment' in news:
                    print(f"        æƒ…æ„Ÿ: {news['sentiment']}, å¾—åˆ†: {news['sentiment_score']}")
        
        print("-" * 50)
    
    print("\n=== æµ‹è¯•æ€»ç»“ ===")
    print("âœ… èˆ†æƒ…åˆ†ææ¨¡å—åŸºæœ¬åŠŸèƒ½æµ‹è¯•å®Œæˆ")
    print("ğŸ“ˆ åç»­å¼€å‘æ–¹å‘:")
    print("   1. é›†æˆåˆ°T01è¯„åˆ†ä½“ç³»")
    print("   2. ä¼˜åŒ–æƒ…æ„Ÿåˆ†æç®—æ³•ï¼ˆä½¿ç”¨NLPæ¨¡å‹ï¼‰")
    print("   3. å¢åŠ æ›´å¤šæ•°æ®æºï¼ˆç¤¾äº¤åª’ä½“ã€è®ºå›ç­‰ï¼‰")
    print("   4. å®æ—¶ç›‘æ§å’Œé¢„è­¦åŠŸèƒ½")


if __name__ == "__main__":
    # æ£€æŸ¥Tavily APIå¯†é’¥
    api_key = os.environ.get('TAVILY_API_KEY')
    if not api_key:
        print("âŒ é”™è¯¯: TAVILY_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        print("è¯·æ‰§è¡Œ: export TAVILY_API_KEY=your_api_key")
        sys.exit(1)
    
    print(f"âœ… TAVILY_API_KEY å·²è®¾ç½®: {api_key[:15]}...")
    
    # è¿è¡Œæµ‹è¯•
    test_with_sample_data()